---
title: 'Bakery Transaction Data Analysis'
author: 'Edward Yu'
date: '2018-11-17'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: readable
    highlight: tango
---

# Introduction
What we have here is a few months worth of sales data from a bakery. 

The original dataset was put on Kaggle but deleted: https://www.kaggle.com/deleted-dataset/50231.

Fortunately someone reuploaded the data: https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery.

What we'll be doing in my first kernel is a little exploratory data analysis (EDA) followed by a Market Basket Analysis (MBA).

## Load packages, check data
Let's install the required packages we'll be using.

```{r, message=FALSE,warning=FALSE}
if (!require(tibble)) install.packages("tibble")
library(tibble)
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if (!require(lubridate)) install.packages("lubridate")
library(lubridate)
if (!require(viridis)) install.packages("viridis")
library(viridis)
if (!require(ggthemes)) install.packages("ggthemes")
library(ggthemes)
if (!require(gridExtra)) install.packages("gridExtra")
library(gridExtra)
if (!require(ggridges)) install.packages("ggridges")
library(ggridges)
if (!require(arules)) install.packages("arules")
library(arules)
if (!require(arulesViz)) install.packages("arulesViz")
library(arulesViz)
```

And get an idea of what our data holds, and the structure of the data.
```{r,warning=FALSE}
x <- read.csv("data/BreadBasket_DMS.csv") %>%
  mutate(Date=as.Date(Date),
         Time=hms(Time)
         )
summary(x)
glimpse(x)
```

Right off the bat we see our time period is relatively narrow: taken between the dates of 2016-10-30 and 2017-04-09. 

In other words, we have only 161 days worth of data.
```{r}
difftime(x$Date[nrow(x)],x$Date[1])
```

We also see most sales happen around 12:45pm and coffee is by far the most popular item.

# Exploration
Let's see what we can glean looking at various manipulations of the data we have available.

## Item frequency
```{r}
x1 <- x %>% 
  group_by(Item) %>% 
  summarise(Count = n()) %>% 
  arrange(desc(Count)) %>%
  slice(1:10) %>% # keep the top 10 items
  ggplot(aes(x=reorder(Item,Count),y=Count,fill=Item))+
  geom_bar(stat="identity")+
  coord_flip()+
  ggtitle("Most popular line items")+
  theme(legend.position="none")
x1
```

No real surprises with frequency, although the `NONE` is slightly odd.
It should be noted that when using `count` we're adding all instances of that particular item. We are not discriminating between different sales receipts. So if one person has ordered 5 coffees, we count coffee 5 times rather than once. Right now this doesn't bother me, we can look at it later on.

# Sales over time
We have an idea of what we're selling so now let's plot it against our various measures of time. Each transaction lists both time and date, we'll first take a large step back and look at sales over all time, followed by a closer daily look.

## All time
```{r}
# Line items sold per day
x2 <- x %>% 
  group_by(Date) %>% 
  summarise(Count= n()) %>% 
  mutate(Day=wday(Date,label=T)) %>% 
  ggplot(aes(x=Date,y=Count,fill=Day))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Line items sold per day")+
  scale_fill_viridis(discrete=TRUE)
x2
```

Plotting total line items sold per day shows some obvious peaks and valleys. Saturday is the big sales day for this bakery, followed by Sunday.

## Weekly
Let's zoom in a little bit to take a closer look. Let's try a weekly view by stacking total sales based on weekday sold.

```{r}
# Total line items sold per weekday
x3 <- x %>% 
  mutate(Day = wday(Date,label=T)) %>% 
  group_by(Day) %>% 
  summarise(Count= n()) %>% 
  ggplot(aes(x=Day,y=Count,fill=Day))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Total line items sold per weekday")+
  theme(legend.position="none")
x3
```

We confirm the general pattern we saw above: our top sales days are on Saturdays, with Sundays and Fridays following behind. Mondays through Thursdays are all roughly similar in total line items sold. Essentially, we've averaged all values creating less drastic peaks as we saw above.

Maybe we should try making the differences more drastic for easier comparison?
One approach might be to filter for top sales days first, and then add all total line items sold by weekday.

```{r}
# Top 50 transactions days per weekday
x6 <- x %>% 
  group_by(Date) %>% 
  summarise(Count= n()) %>% 
  arrange(desc(Count)) %>% 
  slice(1:50) %>% 
  mutate(wday = wday(Date,label=T)) %>% 
  ggplot(aes(x=wday,y=Count,fill=wday))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Top 50 sales days by weekday")+
  theme(legend.position="none")
x6
```

It's clear that Saturdays are much more likely to be top sales days, followed by Sundays and Fridays. This graph could be slightly misleading though and if not interepreted correctly could lead to some false conclusions about how slow weekdays are when compared to the weekends (they aren't as slow as this comparison makes out).

Let's change direction. Remember we talked about line items versus unique sales? Let's try filtering for distinct transactions and see how that changes things.

```{r}
# Total unique transactions per weekday
x4 <- x %>% 
  mutate(wday=wday(Date,label=T)) %>% 
  group_by(wday,Transaction) %>% 
  summarise(n_distinct(Transaction)) %>% 
  summarise(Count=n()) %>% 
  ggplot(aes(x=wday,y=Count,fill=wday))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Unique transactions per weekday")+
  theme(legend.position="none")
x4
```

This is a more fare representation of 'business' throughout the week. Saturday is still the top dog. Sunday falls behind Friday, however, in contrast to what we saw above comparing line items. We might guess that on Sunday we have more generous customers buying for their friends, or even that Sunday tends to be a day when customers buy more than 1 or 2 items.

That does bring to mind a good question, how many items to people purchase on average? 
```{r}
x5.1 <- x %>% 
  mutate(Day = wday(Date,label=T)) %>% 
  group_by(Day) %>% 
  summarise(Count= n()) 

x5.2 <- x %>% 
  mutate(wday=wday(Date,label=T)) %>% 
  group_by(wday,Transaction) %>% 
  summarise(n_distinct(Transaction)) %>% 
  summarise(Count=n())

x5.3 <- data.frame(x5.1, # Days, total items
                   x5.2[2], # unique transactions
                   x5.1[2]/x5.2[2])  # items per unique transaction
colnames(x5.3) <- c("Day","Line","Unique","Items.Trans")

ggplot(x5.3,aes(x=Day,y=Items.Trans,fill=Day))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Total unique transactions per weekday")+
  theme(legend.position="none")+
  geom_text(aes(label=round(Items.Trans,1)), vjust=2)

```

Indeed, people tend to buy more items on Sundays! We also have a slight increase for Saturdays as well when compared to the rest of the week.

Let's zoom in further.

## Hourly
We do a simple line items sold per hour plot and unsurprisingly see that most items are sold around lunch time.
```{r}
# Line items sold per Hour
x7 <- x %>%
  mutate(Hour = as.factor(hour(x$Time))) %>% 
  group_by(Hour) %>% 
  summarise(Count=n()) %>% 
  ggplot(aes(x=Hour,y=Count,fill=Hour))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Line items sold per Hour")+
  theme(legend.position="none")
x7
```

Can we learn anything about unique transactions, as we did above with the date data?
```{r}
# Unique transactions per Hour
x8 <- x %>% 
  mutate(Hour = as.factor(hour(Time)))%>% 
  group_by(Hour,Transaction) %>% 
  summarise(n_distinct(Transaction)) %>% 
  summarise(Count=n()) %>% 
  ggplot(aes(x=Hour,y=Count,fill=Hour))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Unique transactions per Hour")+
  theme(legend.position="none")
x8
```

We really don't see much of a differece besides scale. Again, let's see how many items are bought with each transaction.
```{r}
x9.1 <- x %>% 
  mutate(Hour = as.factor(hour(Time)))%>% 
  group_by(Hour) %>% 
  summarise(Count= n()) 

x9.2 <- x %>% 
  mutate(Hour = as.factor(hour(Time)))%>% 
  group_by(Hour,Transaction) %>% 
  summarise(n_distinct(Transaction)) %>% 
  summarise(Count=n())

x9.3 <- data.frame(x9.1, # Days, total items
                   x9.2[2], # unique transactions
                   x9.1[2]/x9.2[2])  # items per unique transaction
colnames(x9.3) <- c("Hour","Line","Unique","Items.Trans")

x9 <- 
  ggplot(x9.3,aes(x=Hour,y=Items.Trans,fill=Hour))+
  theme_fivethirtyeight()+
  geom_bar(stat="identity")+
  ggtitle("Total items per transaction per hour")+
  theme(legend.position="none")+
  geom_text(aes(label=round(Items.Trans,1)), vjust=2)
x9
```

People buy the most items per transaction between 10am and 5pm, these seem likes standard hours people might walk in to grab a bite to eat. Before 10am most are likely buying a single item like coffee. Similarly with after 5pm.

## Combination
Let's see if we can visualize transaction density per hour for all days of the week using a ridge plot

```{r,message=FALSE}
x10 <- x %>% 
  mutate(Hour=as.factor(hour(Time)),
         Day=wday(Date,label=T)) %>% 
  group_by(Day,Hour,Transaction) %>% 
  summarise(Count=n()) %>% 
  ggplot(aes(x=Hour,y=Day,group=Day,fill=..density..))+
  theme_fivethirtyeight()+
  ggtitle("Transaction density per Hour by Day")+
  theme(legend.position="none")+
  geom_density_ridges_gradient(scale=2, rel_min_height=0.01)+
  scale_fill_viridis(option="magma")
x10
```

The ridge plot gives a nice summary of what we've seen in the above bar charts without overly distorting scales. 

Saturday has a wider breadth and sees most transactions overall.

Sunday falls close being with Sunday lunch having the most dense transaction peak.

It's time for a new take on the data using Market Basket Analysis.

# Market Basket Analysis
Let's import our transaction data to the appropriate format and check it out.

## Import and peak
```{r}
y <- read.transactions("data/BreadBasket_DMS.csv",
                       format="single",
                       cols=c(3,4),
                       sep=","
                       )
summary(y)
```

We see our most frequent items agree with what we found above. And that most transactions are either one or two items, including 3 lucky people who apparently bought for the entire office.

If we want to visualize the frequent items using `arules` we can.
We need to import our transaction data, and we can go ahead and reaffirm our previous findings by using the `itemFrequencyPlot` function to find the top 5 most popular items
```{r}
par(mfrow=c(1,2))
itemFrequencyPlot(y,topN=5,type="relative")
itemFrequencyPlot(y,topN=5,type="absolute")
```

## Support and Confidence
Now before we perform our MBA we need to determine reasonable values for the support and confidence values. 

__Support:__ An itemset with high support means it appears in our transactions frequently. High confidence is direclty related to the frequency an item appears. For example, coffee probably has very high support. 

__Confidence:_*__ Is the strength of our association rule. For example, a confidence of 1 implies that when the LHS item is purchased, the RHS item is purchased 100% of the time.

So let's pin down our `support` value first. 
```{r}
summary(itemFrequency(y))
```

Remember, item frequency is directly related to support. We can see the distribution of values above, with the max frequency at 0.4820079. This is probably coffee which we saw above appears greater than 40% of the time in purchases. 

Since our maximum support is 48%, let's pick a value around 30% to start. We can find this value by dividing 30 by the total amount of transactions.
```{r}
30/length(y)
```

## Assign rules
Our support value will be roughly 0.0045. So what about confidence? Let's use a cointoss as inspiration. Remember that confidence is the likelihood, having purchased the item on the LHS, that the item on the RHS will be purchased in addition. Let's go with 50%, or a value of 0.5.

```{r}
rules <- apriori(y,parameter=list(support=0.0045,confidence=.5))
rules
```

## Inspect rules

We're give a set of 20 rules, which is quite managable. Let's take a look.

```{r}
inspect(head(rules,by="support",n=10))
```

Cake, pastry, and sandwich are most commonly bought in tandem with Coffee. Oddly, Juice is also frequently bought with Coffee... We can only assume that this is due to one person buying for another and hopefully not that some monster is drinking drinking both simultaneously.

```{r}
inspect(head(rules,by="confidence"))
```

More interesting are the association rules. Sorting by confidence gives us items on the LHS that give a high chance of being bought together with the item on the RHS. When looking at purchases that contrain {Cake, Sandwich}, 75% of them also contain {Coffee}.

Considering the suppport or count values, the association of {Toast} => {Coffee} may be more useful.

What about lift?

```{r}
inspect(head(rules,by="lift"))
```

## Lift
The lift measure is related closely to the confidence measure.

__Lift__ is the ratio of *Confidence* to *Expected Confidence* = Number of transactions with RHS / total transactions.

A lift ratio larger than 1.0 implies the relationship between LHS and RHS is significant and not simply chance. The larger the lift ratio, the more significant the relationship between LHS and RHS. Our lift values are not incredibly high, though high enough to rule out random chance.

If we really want to find some high lift values we can refine our ruleset and adjust our support and confidence parameters.
```{r}
rules2 <- apriori(y,parameter=list(support=0.00045,confidence=.9))
inspect(head(rules2,by="lift",n=10))

```

Lift values really rocket up! We would likely be way off an any assumptions made using these association rules as they've only appeared a few times in total (very low support).

Let's get into visualizations now.

## Visualize
There are various plots available from the `arulesViz` library. Though they can be nice to look at I find they aren't quite as useful as tweaking rules manually via the support and confindence values and filtering based on support/confidence/lift. But let's see what we can see.
```{r}
plot(rules, measure=c("support","lift"), shading="confidence")
```

Based on the graph we have one or two points of interest with higher lift than the surrounding points. By adding the `interactive=TRUE` option to the `plot()` function we can select the datapoints for further exploration. It turns out these points are the ones below:
```{r}
inspect(head(rules,by="lift",n=2))
```

Another somewhat useful plot is the Two-key plot
```{r}
plot(rules, method = "two-key plot")
```
We can use this to distinguish total items purchased. In this case we see most people buy only 2 items but 3 item purchases seem to have a higher confidence level.

There are other graphs available in the `arulesViz` library that I simply do not know how to properly use, and in some cases are not useful to us in this particular analysis.

```{r}
plot(rules, method="graph")
```
The above graph is not useful in our situation but would have allowed us to visualize clusters of different RHS items if they existed. In our case almost everyone buys coffee with another item, so Coffee is the only clustered item. In a grocery store we might see many different clusters characterizing different shoppers.

```{r}
plot(rules, method="grouped")
```
```{r}
plot(rules, method="paracoord", control=list(reorder=TRUE))
```

# Conclusions
Taking a cursory look at the data and doing some relatively simple graphical analyses, we were able to discern which days were busiest, and at what times throughout the day that most transactions were taking place.
```{r}
x10
```

This data might be useful when scheduling staff and determining when additional staff is required, or when only minimal staff is needed. Of course, the bakery had probably already figured this out in a couple weeks of business. The graphical representation does give it a nice concrete feeling.

More useful is the number or items per transaction throughout the day.
```{r}
x9
````

As a business we obviously want to sell more items. Some potential lines of action might be to incentivise the purhcase of additional items before 10am and after 5pm. Between the hours of 10am and 5pm we want to incentivise purchasing a 3rd item.

The Market Basket Analysis shows that {Toast} => {Coffee} is a common occurence. Also that {Cake, Sandwich} => {Coffee} has high lift and confidence but low support. We might increase the likelihood of a 3rd item purchased by introducing a Toast/Cake/Coffee or a Sandwich/Cake/Coffee special at discounted price.

